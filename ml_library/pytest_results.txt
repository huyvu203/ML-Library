============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-7.4.4, pluggy-1.6.0 -- /home/huy/.cache/pypoetry/virtualenvs/ml-library-9kDGecR2-py3.12/bin/python
cachedir: .pytest_cache
hypothesis profile 'default'
rootdir: /home/huy/projects/Production-Ready-ML-Library/ml_library
configfile: pyproject.toml
testpaths: tests
plugins: hypothesis-6.135.14, cov-3.0.0
collecting ... collected 76 items

tests/integration/test_pipeline.py::test_regression_pipeline FAILED      [  1%]
tests/integration/test_pipeline.py::test_classification_pipeline FAILED  [  2%]
tests/integration/test_pipeline.py::test_advanced_regression_pipeline[model0] FAILED [  3%]
tests/integration/test_pipeline.py::test_advanced_regression_pipeline[model1] FAILED [  5%]
tests/integration/test_pipeline.py::test_advanced_classification_pipeline[model0] FAILED [  6%]
tests/integration/test_pipeline.py::test_advanced_classification_pipeline[model1] FAILED [  7%]
tests/unit/test_config.py::test_load_yaml PASSED                         [  9%]
tests/unit/test_config.py::test_load_json PASSED                         [ 10%]
tests/unit/test_config.py::test_load_method PASSED                       [ 11%]
tests/unit/test_config.py::test_load_nonexistent_file PASSED             [ 13%]
tests/unit/test_config.py::test_load_unsupported_extension PASSED        [ 14%]
tests/unit/test_config.py::test_load_invalid_yaml PASSED                 [ 15%]
tests/unit/test_config.py::test_load_invalid_json PASSED                 [ 17%]
tests/unit/test_knn_models.py::TestKNNModels::test_knn_regressor_init PASSED [ 18%]
tests/unit/test_knn_models.py::TestKNNModels::test_knn_classifier_init PASSED [ 19%]
tests/unit/test_knn_models.py::test_knn_regressor_get_params[3-uniform-auto] PASSED [ 21%]
tests/unit/test_knn_models.py::test_knn_regressor_get_params[5-distance-ball_tree] PASSED [ 22%]
tests/unit/test_knn_models.py::test_knn_regressor_get_params[10-uniform-kd_tree] PASSED [ 23%]
tests/unit/test_knn_models.py::test_knn_classifier_get_params[3-uniform-auto] PASSED [ 25%]
tests/unit/test_knn_models.py::test_knn_classifier_get_params[5-distance-ball_tree] PASSED [ 26%]
tests/unit/test_knn_models.py::test_knn_classifier_get_params[10-uniform-kd_tree] PASSED [ 27%]
tests/unit/test_knn_models.py::test_knn_regressor_fit_predict PASSED     [ 28%]
tests/unit/test_knn_models.py::test_knn_classifier_fit_predict PASSED    [ 30%]
tests/unit/test_knn_models.py::test_knn_regressor_not_fitted_error PASSED [ 31%]
tests/unit/test_knn_models.py::test_knn_classifier_not_fitted_error PASSED [ 32%]
tests/unit/test_knn_models.py::test_validate_data[KNNRegressor-kwargs0] PASSED [ 34%]
tests/unit/test_knn_models.py::test_validate_data[KNNRegressor-kwargs1] PASSED [ 35%]
tests/unit/test_knn_models.py::test_validate_data[KNNClassifier-kwargs2] PASSED [ 36%]
tests/unit/test_knn_models.py::test_validate_data[KNNClassifier-kwargs3] PASSED [ 38%]
tests/unit/test_models.py::test_linear_regression_initialization PASSED  [ 39%]
tests/unit/test_models.py::test_linear_regression_fit_predict FAILED     [ 40%]
tests/unit/test_models.py::test_linear_regression_score FAILED           [ 42%]
tests/unit/test_models.py::test_linear_regression_not_fitted_error PASSED [ 43%]
tests/unit/test_models.py::test_linear_regression_get_params PASSED      [ 44%]
tests/unit/test_models.py::test_logistic_regression_initialization PASSED [ 46%]
tests/unit/test_models.py::test_logistic_regression_fit_predict FAILED   [ 47%]
tests/unit/test_models.py::test_logistic_regression_predict_proba FAILED [ 48%]
tests/unit/test_models.py::test_logistic_regression_score FAILED         [ 50%]
tests/unit/test_models.py::test_logistic_regression_not_fitted_error PASSED [ 51%]
tests/unit/test_models.py::test_logistic_regression_get_params PASSED    [ 52%]
tests/unit/test_svm_imports.py::test_svm_imports PASSED                  [ 53%]
tests/unit/test_svm_models.py::TestSVMModels::test_svm_regressor_init PASSED [ 55%]
tests/unit/test_svm_models.py::TestSVMModels::test_svm_classifier_init PASSED [ 56%]
tests/unit/test_svm_models.py::test_svm_regressor_get_params[linear-1.0-scale] PASSED [ 57%]
tests/unit/test_svm_models.py::test_svm_regressor_get_params[rbf-0.5-auto] PASSED [ 59%]
tests/unit/test_svm_models.py::test_svm_regressor_get_params[poly-2.0-0.1] PASSED [ 60%]
tests/unit/test_svm_models.py::test_svm_classifier_get_params[linear-1.0-scale] PASSED [ 61%]
tests/unit/test_svm_models.py::test_svm_classifier_get_params[rbf-0.5-auto] PASSED [ 63%]
tests/unit/test_svm_models.py::test_svm_classifier_get_params[poly-2.0-0.1] PASSED [ 64%]
tests/unit/test_svm_models.py::test_svm_regressor_fit_predict PASSED     [ 65%]
tests/unit/test_svm_models.py::test_svm_classifier_fit_predict PASSED    [ 67%]
tests/unit/test_svm_models.py::test_svm_regressor_not_fitted_error PASSED [ 68%]
tests/unit/test_svm_models.py::test_svm_classifier_not_fitted_error PASSED [ 69%]
tests/unit/test_svm_models.py::test_svm_classifier_probability_error PASSED [ 71%]
tests/unit/test_tree_models.py::TestTreeModels::test_decision_tree_regressor_init PASSED [ 72%]
tests/unit/test_tree_models.py::TestTreeModels::test_decision_tree_classifier_init PASSED [ 73%]
tests/unit/test_tree_models.py::TestTreeModels::test_random_forest_regressor_init PASSED [ 75%]
tests/unit/test_tree_models.py::TestTreeModels::test_random_forest_classifier_init PASSED [ 76%]
tests/unit/test_tree_models.py::TestTreeModels::test_xgboost_regressor_init PASSED [ 77%]
tests/unit/test_tree_models.py::TestTreeModels::test_xgboost_classifier_init PASSED [ 78%]
tests/unit/test_tree_models.py::test_regressor_fit_predict[DecisionTreeRegressor-model_params0] PASSED [ 80%]
tests/unit/test_tree_models.py::test_regressor_fit_predict[RandomForestRegressor-model_params1] PASSED [ 81%]
tests/unit/test_tree_models.py::test_regressor_fit_predict[XGBoostRegressor-model_params2] PASSED [ 82%]
tests/unit/test_tree_models.py::test_classifier_fit_predict[DecisionTreeClassifier-model_params0] PASSED [ 84%]
tests/unit/test_tree_models.py::test_classifier_fit_predict[RandomForestClassifier-model_params1] PASSED [ 85%]
tests/unit/test_tree_models.py::test_classifier_fit_predict[XGBoostClassifier-model_params2] PASSED [ 86%]
tests/unit/temp/test_imports.py::test_base_imports PASSED                [ 88%]
tests/unit/temp/test_imports.py::test_linear_models PASSED               [ 89%]
tests/unit/temp/test_imports.py::test_tree_models PASSED                 [ 90%]
tests/unit/temp/test_imports.py::test_boosting_models PASSED             [ 92%]
tests/unit/temp/test_imports.py::test_svm_models PASSED                  [ 93%]
tests/unit/temp/test_imports.py::test_knn_models PASSED                  [ 94%]
tests/unit/temp/test_svm.py::test_svm_regressor_init PASSED              [ 96%]
tests/unit/temp/test_svm.py::test_svm_classifier_init PASSED             [ 97%]
tests/unit/temp/test_svm.py::test_svm_regressor_fit_predict PASSED       [ 98%]
tests/unit/temp/test_svm.py::test_svm_classifier_fit_predict PASSED      [100%]

=================================== FAILURES ===================================
___________________________ test_regression_pipeline ___________________________

sample_regression_data = (array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864],
       [0.15599452, 0.05808361, 0.86617615, 0.60...49334,
        2.15238086,  1.23163574,  2.9915195 ,  0.89809422,  1.95549099]), array([ 3.5,  1.7, -4.2,  2.1, -1.3]))

    def test_regression_pipeline(sample_regression_data):
        """Test the full regression pipeline from training to evaluation."""
        X, y, _ = sample_regression_data
    
        # Split data
        n_samples = len(X)
        indices = np.random.permutation(n_samples)
        test_size = int(0.2 * n_samples)
        test_indices = indices[:test_size]
        train_indices = indices[test_size:]
    
        X_train, X_test = X[train_indices], X[test_indices]
        y_train, y_test = y[train_indices], y[test_indices]
    
        # Create model
        model = LinearRegression(learning_rate=0.01, n_iterations=1000)
    
        # Train the model
        trainer = Trainer(model, validation_split=0.0)
>       trained_model = trainer.train(X_train, y_train)

tests/integration/test_pipeline.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ml_library/training/trainer.py:87: in train
    self.model.fit(X_train, y_train)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ml_library.models.regression.LinearRegression object at 0x7ed81ff41d90>
X = array([[0.01215447, 0.96987883, 0.04315991, 0.89114311, 0.52770111],
       [0.54269608, 0.14092422, 0.80219698, 0.074... 0.89721576, 0.90041806, 0.63310146, 0.33902979],
       [0.38610264, 0.96119056, 0.90535064, 0.19579113, 0.0693613 ]])
y = array([[ 2.63149334],
       [-1.73770836],
       [ 1.85909416],
       [ 0.89809422],
       [ 2.90285312],
       [...445963],
       [-1.6206308 ],
       [ 2.42251447],
       [ 0.5240389 ],
       [-0.71916706],
       [-1.09503676]])

    def fit(self, X: np.ndarray, y: np.ndarray) -> "LinearRegression":
        """Fit the linear model using gradient descent.
    
        Args:
            X: Training data of shape (n_samples, n_features)
            y: Target values of shape (n_samples,)
    
        Returns:
            self: The fitted model
        """
        X = self._validate_data(X)
        y = np.array(y).reshape(-1, 1)
    
        n_samples, n_features = X.shape
    
        # Initialize parameters
        if self.fit_intercept:
            # Add a column of ones for the intercept
            X_with_intercept = np.hstack((np.ones((n_samples, 1)), X))
            self.weights_ = np.zeros(n_features + 1)
        else:
            X_with_intercept = X
            self.weights_ = np.zeros(n_features)
    
        # Gradient Descent
        prev_cost = float('inf')
        for i in range(self.n_iterations):
            # Calculate predictions
            y_pred = np.dot(X_with_intercept, self.weights_)
    
            # Calculate the error
            error = y_pred - y
    
            # Calculate the gradient
            gradient = (1/n_samples) * np.dot(X_with_intercept.T, error)
    
            # Update parameters
>           self.weights_ -= self.learning_rate * gradient.flatten()
E           ValueError: operands could not be broadcast together with shapes (6,) (480,) (6,)

ml_library/models/regression.py:87: ValueError
----------------------------- Captured stderr call -----------------------------
2025-06-23 01:15:47 | DEBUG    | ml_library.models.base:__init__:29 | Initialized LinearRegression with params: {'fit_intercept': True, 'learning_rate': 0.01, 'n_iterations': 1000, 'tol': 0.0001}
2025-06-23 01:15:47 | DEBUG    | ml_library.training.trainer:__init__:41 | Initialized trainer with model LinearRegression, validation_split=0.0, random_state=None, params={}
2025-06-23 01:15:47 | INFO     | ml_library.training.trainer:train:64 | Starting model training
2025-06-23 01:15:47 | INFO     | ml_library.training.trainer:train:86 | Fitting model
_________________________ test_classification_pipeline _________________________

sample_classification_data = (array([[-1.25459881,  4.50714306],
       [ 2.31993942,  0.98658484],
       [-3.4398136 , -3.4400548 ],
       [-4.4...,
       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1]))

    def test_classification_pipeline(sample_classification_data):
        """Test the full classification pipeline from training to evaluation."""
        X, y = sample_classification_data
    
        # Split data
        n_samples = len(X)
        indices = np.random.permutation(n_samples)
        test_size = int(0.2 * n_samples)
        test_indices = indices[:test_size]
        train_indices = indices[test_size:]
    
        X_train, X_test = X[train_indices], X[test_indices]
        y_train, y_test = y[train_indices], y[test_indices]
    
        # Create model
        model = LogisticRegression(learning_rate=0.1, n_iterations=1000)
    
        # Train the model
        trainer = Trainer(model, validation_split=0.0)
>       trained_model = trainer.train(X_train, y_train)

tests/integration/test_pipeline.py:78: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ml_library/training/trainer.py:87: in train
    self.model.fit(X_train, y_train)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ml_library.models.classification.LogisticRegression object at 0x7ed81ffb00e0>
X = array([[ 0.92414569, -4.53549587],
       [-3.80405754,  2.13244787],
       [-1.32216867,  1.32305831],
       [-4.93...-3.01284318],
       [-0.02751494, -1.9912169 ],
       [-4.48521249, -2.21353536],
       [-1.88288924,  0.20068021]])
y = array([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
       1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,...    0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
       0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0])

    def fit(self, X: np.ndarray, y: np.ndarray) -> "LogisticRegression":
        """Fit the logistic regression model using gradient descent.
    
        Args:
            X: Training data of shape (n_samples, n_features)
            y: Target values of shape (n_samples,)
    
        Returns:
            self: The fitted model
        """
        X = self._validate_data(X)
        y = np.array(y)
    
        # Get unique classes
        self.classes_ = np.unique(y)
    
        if len(self.classes_) != 2:
            raise ValueError(
                f"This LogisticRegression implementation only supports binary classification. "
                f"Found {len(self.classes_)} classes."
            )
    
        # Map classes to 0 and 1
        y_binary = np.where(y == self.classes_[1], 1, 0)
        y_binary = y_binary.reshape(-1, 1)
    
        n_samples, n_features = X.shape
    
        # Initialize parameters
        if self.fit_intercept:
            # Add a column of ones for the intercept
            X_with_intercept = np.hstack((np.ones((n_samples, 1)), X))
            self.weights_ = np.zeros(n_features + 1)
        else:
            X_with_intercept = X
            self.weights_ = np.zeros(n_features)
    
        # Gradient Descent
        prev_cost = float('inf')
        for i in range(self.n_iterations):
            # Calculate predictions (probability of class 1)
            linear_model = np.dot(X_with_intercept, self.weights_)
            y_pred = sigmoid(linear_model)
    
            # Calculate error
            error = y_pred - y_binary
    
            # Calculate gradients
            gradient = (1/n_samples) * np.dot(X_with_intercept.T, error)
    
            # Add regularization term if specified
            if self.penalty == "l2":
                # L2 regularization (exclude intercept from regularization)
                if self.fit_intercept:
                    reg_term = np.zeros_like(self.weights_)
                    reg_term[1:] = self.weights_[1:] / self.C
                else:
                    reg_term = self.weights_ / self.C
                gradient += reg_term
            elif self.penalty == "l1":
                # L1 regularization (exclude intercept from regularization)
                if self.fit_intercept:
                    reg_term = np.zeros_like(self.weights_)
                    reg_term[1:] = np.sign(self.weights_[1:]) / self.C
                else:
                    reg_term = np.sign(self.weights_) / self.C
                gradient += reg_term
    
            # Update parameters
>           self.weights_ -= self.learning_rate * gradient.flatten()
E           ValueError: operands could not be broadcast together with shapes (3,) (240,) (3,)

ml_library/models/classification.py:146: ValueError
----------------------------- Captured stderr call -----------------------------
2025-06-23 01:15:47 | DEBUG    | ml_library.models.base:__init__:29 | Initialized LogisticRegression with params: {'learning_rate': 0.1, 'n_iterations': 1000, 'tol': 0.0001, 'fit_intercept': True, 'penalty': 'none', 'C': 1.0}
2025-06-23 01:15:47 | DEBUG    | ml_library.training.trainer:__init__:41 | Initialized trainer with model LogisticRegression, validation_split=0.0, random_state=None, params={}
2025-06-23 01:15:47 | INFO     | ml_library.training.trainer:train:64 | Starting model training
2025-06-23 01:15:47 | INFO     | ml_library.training.trainer:train:86 | Fitting model
__________________ test_advanced_regression_pipeline[model0] ___________________

self = <ml_library.inference.predictor.Predictor object at 0x7ed822992f60>
model = <ml_library.models.svm_models.SVMRegressor object at 0x7ed8209e86b0>

    def __init__(self, model: BaseModel) -> None:
        """Initialize the Predictor.
    
        Args:
            model: The trained model to use for predictions
    
        Raises:
            ValueError: If the model is not trained
        """
        self.model = model
    
        # Check if model is trained
        try:
>           self.model.check_is_fitted()
E           AttributeError: 'SVMRegressor' object has no attribute 'check_is_fitted'

ml_library/inference/predictor.py:33: AttributeError

During handling of the above exception, another exception occurred:

sample_regression_data = (array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864],
       [0.15599452, 0.05808361, 0.86617615, 0.60...49334,
        2.15238086,  1.23163574,  2.9915195 ,  0.89809422,  1.95549099]), array([ 3.5,  1.7, -4.2,  2.1, -1.3]))
model = <ml_library.models.svm_models.SVMRegressor object at 0x7ed8209e86b0>

    @pytest.mark.parametrize(
        "model",
        [
            SVMRegressor(kernel="linear", C=1.0),
            KNNRegressor(n_neighbors=3)
        ]
    )
    def test_advanced_regression_pipeline(sample_regression_data, model):
        """Test regression pipeline with SVM and KNN regression models."""
        X, y, _ = sample_regression_data
    
        # Split data
        n_samples = len(X)
        indices = np.random.permutation(n_samples)
        test_size = int(0.2 * n_samples)
        test_indices = indices[:test_size]
        train_indices = indices[test_size:]
    
        X_train, X_test = X[train_indices], X[test_indices]
        y_train, y_test = y[train_indices], y[test_indices]
    
        # Train the model
        trainer = Trainer(model, validation_split=0.2)
        trained_model = trainer.train(X_train, y_train)
    
        # Get metrics
        metrics = trainer.get_metrics()
        assert "train_score" in metrics
        assert "val_score" in metrics
    
        # Create predictor
>       predictor = Predictor(trained_model)

tests/integration/test_pipeline.py:140: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ml_library.inference.predictor.Predictor object at 0x7ed822992f60>
model = <ml_library.models.svm_models.SVMRegressor object at 0x7ed8209e86b0>

    def __init__(self, model: BaseModel) -> None:
        """Initialize the Predictor.
    
        Args:
            model: The trained model to use for predictions
    
        Raises:
            ValueError: If the model is not trained
        """
        self.model = model
    
        # Check if model is trained
        try:
            self.model.check_is_fitted()
        except (ValueError, AttributeError) as e:
            logger.error(f"Model is not properly trained: {e}")
>           raise ValueError("Model must be trained before creating a predictor")
E           ValueError: Model must be trained before creating a predictor

ml_library/inference/predictor.py:36: ValueError
----------------------------- Captured stderr call -----------------------------
2025-06-23 01:15:47 | DEBUG    | ml_library.training.trainer:__init__:41 | Initialized trainer with model SVMRegressor, validation_split=0.2, random_state=None, params={}
2025-06-23 01:15:47 | INFO     | ml_library.training.trainer:train:64 | Starting model training
2025-06-23 01:15:47 | DEBUG    | ml_library.training.trainer:train:71 | Split data for validation: X_train: (64, 5), y_train: (64,), X_val: (16, 5), y_val: (16,)
2025-06-23 01:15:47 | INFO     | ml_library.training.trainer:train:86 | Fitting model
2025-06-23 01:15:47 | DEBUG    | ml_library.models.svm_models:fit:89 | Fitting SVMRegressor with kernel=linear, C=1.0, epsilon=0.1, gamma=scale
2025-06-23 01:15:47 | INFO     | ml_library.models.svm_models:fit:110 | SVMRegressor fitted successfully with 56 support vectors
2025-06-23 01:15:47 | INFO     | ml_library.training.trainer:train:92 | Training score: 0.8988
2025-06-23 01:15:47 | INFO     | ml_library.training.trainer:train:98 | Validation score: 0.8121
2025-06-23 01:15:47 | ERROR    | ml_library.inference.predictor:__init__:35 | Model is not properly trained: 'SVMRegressor' object has no attribute 'check_is_fitted'
__________________ test_advanced_regression_pipeline[model1] ___________________

self = <ml_library.inference.predictor.Predictor object at 0x7ed820536960>
model = <ml_library.models.knn_models.KNNRegressor object at 0x7ed822fc2180>

    def __init__(self, model: BaseModel) -> None:
        """Initialize the Predictor.
    
        Args:
            model: The trained model to use for predictions
    
        Raises:
            ValueError: If the model is not trained
        """
        self.model = model
    
        # Check if model is trained
        try:
>           self.model.check_is_fitted()
E           AttributeError: 'KNNRegressor' object has no attribute 'check_is_fitted'

ml_library/inference/predictor.py:33: AttributeError

During handling of the above exception, another exception occurred:

sample_regression_data = (array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864],
       [0.15599452, 0.05808361, 0.86617615, 0.60...49334,
        2.15238086,  1.23163574,  2.9915195 ,  0.89809422,  1.95549099]), array([ 3.5,  1.7, -4.2,  2.1, -1.3]))
model = <ml_library.models.knn_models.KNNRegressor object at 0x7ed822fc2180>

    @pytest.mark.parametrize(
        "model",
        [
            SVMRegressor(kernel="linear", C=1.0),
            KNNRegressor(n_neighbors=3)
        ]
    )
    def test_advanced_regression_pipeline(sample_regression_data, model):
        """Test regression pipeline with SVM and KNN regression models."""
        X, y, _ = sample_regression_data
    
        # Split data
        n_samples = len(X)
        indices = np.random.permutation(n_samples)
        test_size = int(0.2 * n_samples)
        test_indices = indices[:test_size]
        train_indices = indices[test_size:]
    
        X_train, X_test = X[train_indices], X[test_indices]
        y_train, y_test = y[train_indices], y[test_indices]
    
        # Train the model
        trainer = Trainer(model, validation_split=0.2)
        trained_model = trainer.train(X_train, y_train)
    
        # Get metrics
        metrics = trainer.get_metrics()
        assert "train_score" in metrics
        assert "val_score" in metrics
    
        # Create predictor
>       predictor = Predictor(trained_model)

tests/integration/test_pipeline.py:140: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ml_library.inference.predictor.Predictor object at 0x7ed820536960>
model = <ml_library.models.knn_models.KNNRegressor object at 0x7ed822fc2180>

    def __init__(self, model: BaseModel) -> None:
        """Initialize the Predictor.
    
        Args:
            model: The trained model to use for predictions
    
        Raises:
            ValueError: If the model is not trained
        """
        self.model = model
    
        # Check if model is trained
        try:
            self.model.check_is_fitted()
        except (ValueError, AttributeError) as e:
            logger.error(f"Model is not properly trained: {e}")
>           raise ValueError("Model must be trained before creating a predictor")
E           ValueError: Model must be trained before creating a predictor

ml_library/inference/predictor.py:36: ValueError
----------------------------- Captured stderr call -----------------------------
2025-06-23 01:15:47 | DEBUG    | ml_library.training.trainer:__init__:41 | Initialized trainer with model KNNRegressor, validation_split=0.2, random_state=None, params={}
2025-06-23 01:15:47 | INFO     | ml_library.training.trainer:train:64 | Starting model training
2025-06-23 01:15:47 | DEBUG    | ml_library.training.trainer:train:71 | Split data for validation: X_train: (64, 5), y_train: (64,), X_val: (16, 5), y_val: (16,)
2025-06-23 01:15:47 | INFO     | ml_library.training.trainer:train:86 | Fitting model
2025-06-23 01:15:47 | DEBUG    | ml_library.models.knn_models:fit:85 | Fitting KNNRegressor with n_neighbors=3, weights=uniform, algorithm=auto
2025-06-23 01:15:47 | INFO     | ml_library.models.knn_models:fit:104 | KNNRegressor fitted successfully with 64 training samples
2025-06-23 01:15:47 | INFO     | ml_library.training.trainer:train:92 | Training score: 0.8950
2025-06-23 01:15:47 | INFO     | ml_library.training.trainer:train:98 | Validation score: 0.6859
2025-06-23 01:15:47 | ERROR    | ml_library.inference.predictor:__init__:35 | Model is not properly trained: 'KNNRegressor' object has no attribute 'check_is_fitted'
________________ test_advanced_classification_pipeline[model0] _________________

self = <ml_library.inference.predictor.Predictor object at 0x7ed8200b1100>
model = <ml_library.models.svm_models.SVMClassifier object at 0x7ed821667bc0>

    def __init__(self, model: BaseModel) -> None:
        """Initialize the Predictor.
    
        Args:
            model: The trained model to use for predictions
    
        Raises:
            ValueError: If the model is not trained
        """
        self.model = model
    
        # Check if model is trained
        try:
>           self.model.check_is_fitted()
E           AttributeError: 'SVMClassifier' object has no attribute 'check_is_fitted'

ml_library/inference/predictor.py:33: AttributeError

During handling of the above exception, another exception occurred:

sample_classification_data = (array([[-1.25459881,  4.50714306],
       [ 2.31993942,  0.98658484],
       [-3.4398136 , -3.4400548 ],
       [-4.4...,
       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1]))
model = <ml_library.models.svm_models.SVMClassifier object at 0x7ed821667bc0>

    @pytest.mark.parametrize(
        "model",
        [
            SVMClassifier(kernel="linear", C=1.0, probability=True),
            KNNClassifier(n_neighbors=5)
        ]
    )
    def test_advanced_classification_pipeline(sample_classification_data, model):
        """Test classification pipeline with SVM and KNN classification models."""
        X, y = sample_classification_data
    
        # Split data
        n_samples = len(X)
        indices = np.random.permutation(n_samples)
        test_size = int(0.2 * n_samples)
        test_indices = indices[:test_size]
        train_indices = indices[test_size:]
    
        X_train, X_test = X[train_indices], X[test_indices]
        y_train, y_test = y[train_indices], y[test_indices]
    
        # Train the model
        trainer = Trainer(model, validation_split=0.2)
        trained_model = trainer.train(X_train, y_train)
    
        # Get metrics
        metrics = trainer.get_metrics()
        assert "train_score" in metrics
        assert "val_score" in metrics
    
        # Create predictor
>       predictor = Predictor(trained_model)

tests/integration/test_pipeline.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ml_library.inference.predictor.Predictor object at 0x7ed8200b1100>
model = <ml_library.models.svm_models.SVMClassifier object at 0x7ed821667bc0>

    def __init__(self, model: BaseModel) -> None:
        """Initialize the Predictor.
    
        Args:
            model: The trained model to use for predictions
    
        Raises:
            ValueError: If the model is not trained
        """
        self.model = model
    
        # Check if model is trained
        try:
            self.model.check_is_fitted()
        except (ValueError, AttributeError) as e:
            logger.error(f"Model is not properly trained: {e}")
>           raise ValueError("Model must be trained before creating a predictor")
E           ValueError: Model must be trained before creating a predictor

ml_library/inference/predictor.py:36: ValueError
----------------------------- Captured stderr call -----------------------------
2025-06-23 01:15:47 | DEBUG    | ml_library.training.trainer:__init__:41 | Initialized trainer with model SVMClassifier, validation_split=0.2, random_state=None, params={}
2025-06-23 01:15:47 | INFO     | ml_library.training.trainer:train:64 | Starting model training
2025-06-23 01:15:47 | DEBUG    | ml_library.training.trainer:train:71 | Split data for validation: X_train: (64, 2), y_train: (64,), X_val: (16, 2), y_val: (16,)
2025-06-23 01:15:47 | INFO     | ml_library.training.trainer:train:86 | Fitting model
2025-06-23 01:15:47 | DEBUG    | ml_library.models.svm_models:fit:262 | Fitting SVMClassifier with kernel=linear, C=1.0, gamma=scale
2025-06-23 01:15:47 | INFO     | ml_library.models.svm_models:fit:286 | SVMClassifier fitted successfully with 8 support vectors for 2 classes
2025-06-23 01:15:47 | INFO     | ml_library.training.trainer:train:92 | Training score: 1.0000
2025-06-23 01:15:47 | INFO     | ml_library.training.trainer:train:98 | Validation score: 0.8750
2025-06-23 01:15:47 | ERROR    | ml_library.inference.predictor:__init__:35 | Model is not properly trained: 'SVMClassifier' object has no attribute 'check_is_fitted'
________________ test_advanced_classification_pipeline[model1] _________________

self = <ml_library.inference.predictor.Predictor object at 0x7ed8205ad5e0>
model = <ml_library.models.knn_models.KNNClassifier object at 0x7ed820f3b5f0>

    def __init__(self, model: BaseModel) -> None:
        """Initialize the Predictor.
    
        Args:
            model: The trained model to use for predictions
    
        Raises:
            ValueError: If the model is not trained
        """
        self.model = model
    
        # Check if model is trained
        try:
>           self.model.check_is_fitted()
E           AttributeError: 'KNNClassifier' object has no attribute 'check_is_fitted'

ml_library/inference/predictor.py:33: AttributeError

During handling of the above exception, another exception occurred:

sample_classification_data = (array([[-1.25459881,  4.50714306],
       [ 2.31993942,  0.98658484],
       [-3.4398136 , -3.4400548 ],
       [-4.4...,
       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1]))
model = <ml_library.models.knn_models.KNNClassifier object at 0x7ed820f3b5f0>

    @pytest.mark.parametrize(
        "model",
        [
            SVMClassifier(kernel="linear", C=1.0, probability=True),
            KNNClassifier(n_neighbors=5)
        ]
    )
    def test_advanced_classification_pipeline(sample_classification_data, model):
        """Test classification pipeline with SVM and KNN classification models."""
        X, y = sample_classification_data
    
        # Split data
        n_samples = len(X)
        indices = np.random.permutation(n_samples)
        test_size = int(0.2 * n_samples)
        test_indices = indices[:test_size]
        train_indices = indices[test_size:]
    
        X_train, X_test = X[train_indices], X[test_indices]
        y_train, y_test = y[train_indices], y[test_indices]
    
        # Train the model
        trainer = Trainer(model, validation_split=0.2)
        trained_model = trainer.train(X_train, y_train)
    
        # Get metrics
        metrics = trainer.get_metrics()
        assert "train_score" in metrics
        assert "val_score" in metrics
    
        # Create predictor
>       predictor = Predictor(trained_model)

tests/integration/test_pipeline.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ml_library.inference.predictor.Predictor object at 0x7ed8205ad5e0>
model = <ml_library.models.knn_models.KNNClassifier object at 0x7ed820f3b5f0>

    def __init__(self, model: BaseModel) -> None:
        """Initialize the Predictor.
    
        Args:
            model: The trained model to use for predictions
    
        Raises:
            ValueError: If the model is not trained
        """
        self.model = model
    
        # Check if model is trained
        try:
            self.model.check_is_fitted()
        except (ValueError, AttributeError) as e:
            logger.error(f"Model is not properly trained: {e}")
>           raise ValueError("Model must be trained before creating a predictor")
E           ValueError: Model must be trained before creating a predictor

ml_library/inference/predictor.py:36: ValueError
----------------------------- Captured stderr call -----------------------------
2025-06-23 01:15:47 | DEBUG    | ml_library.training.trainer:__init__:41 | Initialized trainer with model KNNClassifier, validation_split=0.2, random_state=None, params={}
2025-06-23 01:15:47 | INFO     | ml_library.training.trainer:train:64 | Starting model training
2025-06-23 01:15:47 | DEBUG    | ml_library.training.trainer:train:71 | Split data for validation: X_train: (64, 2), y_train: (64,), X_val: (16, 2), y_val: (16,)
2025-06-23 01:15:47 | INFO     | ml_library.training.trainer:train:86 | Fitting model
2025-06-23 01:15:47 | DEBUG    | ml_library.models.knn_models:fit:247 | Fitting KNNClassifier with n_neighbors=5, weights=uniform, algorithm=auto
2025-06-23 01:15:47 | INFO     | ml_library.models.knn_models:fit:268 | KNNClassifier fitted successfully with 64 training samples for 2 classes
2025-06-23 01:15:47 | INFO     | ml_library.training.trainer:train:92 | Training score: 0.9531
2025-06-23 01:15:47 | INFO     | ml_library.training.trainer:train:98 | Validation score: 0.9375
2025-06-23 01:15:47 | ERROR    | ml_library.inference.predictor:__init__:35 | Model is not properly trained: 'KNNClassifier' object has no attribute 'check_is_fitted'
______________________ test_linear_regression_fit_predict ______________________

sample_regression_data = (array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864],
       [0.15599452, 0.05808361, 0.86617615, 0.60...49334,
        2.15238086,  1.23163574,  2.9915195 ,  0.89809422,  1.95549099]), array([ 3.5,  1.7, -4.2,  2.1, -1.3]))

    def test_linear_regression_fit_predict(sample_regression_data):
        """Test fitting and prediction of LinearRegression."""
        X, y, true_coef = sample_regression_data
    
        # Create and fit the model
        model = LinearRegression(n_iterations=5000, learning_rate=0.01)
>       model.fit(X, y)

tests/unit/test_models.py:31: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ml_library.models.regression.LinearRegression object at 0x7ed81fdaa780>
X = array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864],
       [0.15599452, 0.05808361, 0.86617615, 0.601... 0.6201326 , 0.27738118, 0.18812116, 0.4636984 ],
       [0.35335223, 0.58365611, 0.07773464, 0.97439481, 0.98621074]])
y = array([[ 1.07796675],
       [-1.71328426],
       [-1.09061502],
       [-0.80481459],
       [ 0.87887873],
       [...149334],
       [ 2.15238086],
       [ 1.23163574],
       [ 2.9915195 ],
       [ 0.89809422],
       [ 1.95549099]])

    def fit(self, X: np.ndarray, y: np.ndarray) -> "LinearRegression":
        """Fit the linear model using gradient descent.
    
        Args:
            X: Training data of shape (n_samples, n_features)
            y: Target values of shape (n_samples,)
    
        Returns:
            self: The fitted model
        """
        X = self._validate_data(X)
        y = np.array(y).reshape(-1, 1)
    
        n_samples, n_features = X.shape
    
        # Initialize parameters
        if self.fit_intercept:
            # Add a column of ones for the intercept
            X_with_intercept = np.hstack((np.ones((n_samples, 1)), X))
            self.weights_ = np.zeros(n_features + 1)
        else:
            X_with_intercept = X
            self.weights_ = np.zeros(n_features)
    
        # Gradient Descent
        prev_cost = float('inf')
        for i in range(self.n_iterations):
            # Calculate predictions
            y_pred = np.dot(X_with_intercept, self.weights_)
    
            # Calculate the error
            error = y_pred - y
    
            # Calculate the gradient
            gradient = (1/n_samples) * np.dot(X_with_intercept.T, error)
    
            # Update parameters
>           self.weights_ -= self.learning_rate * gradient.flatten()
E           ValueError: operands could not be broadcast together with shapes (6,) (600,) (6,)

ml_library/models/regression.py:87: ValueError
----------------------------- Captured stderr call -----------------------------
2025-06-23 01:15:47 | DEBUG    | ml_library.models.base:__init__:29 | Initialized LinearRegression with params: {'fit_intercept': True, 'learning_rate': 0.01, 'n_iterations': 5000, 'tol': 0.0001}
_________________________ test_linear_regression_score _________________________

sample_regression_data = (array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864],
       [0.15599452, 0.05808361, 0.86617615, 0.60...49334,
        2.15238086,  1.23163574,  2.9915195 ,  0.89809422,  1.95549099]), array([ 3.5,  1.7, -4.2,  2.1, -1.3]))

    def test_linear_regression_score(sample_regression_data):
        """Test the score method of LinearRegression."""
        X, y, _ = sample_regression_data
    
        # Create and fit the model
        model = LinearRegression()
>       model.fit(X, y)

tests/unit/test_models.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ml_library.models.regression.LinearRegression object at 0x7ed81fdd6570>
X = array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864],
       [0.15599452, 0.05808361, 0.86617615, 0.601... 0.6201326 , 0.27738118, 0.18812116, 0.4636984 ],
       [0.35335223, 0.58365611, 0.07773464, 0.97439481, 0.98621074]])
y = array([[ 1.07796675],
       [-1.71328426],
       [-1.09061502],
       [-0.80481459],
       [ 0.87887873],
       [...149334],
       [ 2.15238086],
       [ 1.23163574],
       [ 2.9915195 ],
       [ 0.89809422],
       [ 1.95549099]])

    def fit(self, X: np.ndarray, y: np.ndarray) -> "LinearRegression":
        """Fit the linear model using gradient descent.
    
        Args:
            X: Training data of shape (n_samples, n_features)
            y: Target values of shape (n_samples,)
    
        Returns:
            self: The fitted model
        """
        X = self._validate_data(X)
        y = np.array(y).reshape(-1, 1)
    
        n_samples, n_features = X.shape
    
        # Initialize parameters
        if self.fit_intercept:
            # Add a column of ones for the intercept
            X_with_intercept = np.hstack((np.ones((n_samples, 1)), X))
            self.weights_ = np.zeros(n_features + 1)
        else:
            X_with_intercept = X
            self.weights_ = np.zeros(n_features)
    
        # Gradient Descent
        prev_cost = float('inf')
        for i in range(self.n_iterations):
            # Calculate predictions
            y_pred = np.dot(X_with_intercept, self.weights_)
    
            # Calculate the error
            error = y_pred - y
    
            # Calculate the gradient
            gradient = (1/n_samples) * np.dot(X_with_intercept.T, error)
    
            # Update parameters
>           self.weights_ -= self.learning_rate * gradient.flatten()
E           ValueError: operands could not be broadcast together with shapes (6,) (600,) (6,)

ml_library/models/regression.py:87: ValueError
----------------------------- Captured stderr call -----------------------------
2025-06-23 01:15:47 | DEBUG    | ml_library.models.base:__init__:29 | Initialized LinearRegression with params: {'fit_intercept': True, 'learning_rate': 0.01, 'n_iterations': 1000, 'tol': 0.0001}
_____________________ test_logistic_regression_fit_predict _____________________

sample_classification_data = (array([[-1.25459881,  4.50714306],
       [ 2.31993942,  0.98658484],
       [-3.4398136 , -3.4400548 ],
       [-4.4...,
       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1]))

    def test_logistic_regression_fit_predict(sample_classification_data):
        """Test fitting and prediction of LogisticRegression."""
        X, y = sample_classification_data
    
        # Create and fit the model
        model = LogisticRegression(n_iterations=1000, learning_rate=0.1)
>       model.fit(X, y)

tests/unit/test_models.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ml_library.models.classification.LogisticRegression object at 0x7ed81ffb2270>
X = array([[-1.25459881,  4.50714306],
       [ 2.31993942,  0.98658484],
       [-3.4398136 , -3.4400548 ],
       [-4.41... 1.33101457],
       [-1.60970209, -1.50790425],
       [ 2.25955679,  3.9711026 ],
       [ 3.87086424,  2.79875546]])
y = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
       0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,...0,
       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1])

    def fit(self, X: np.ndarray, y: np.ndarray) -> "LogisticRegression":
        """Fit the logistic regression model using gradient descent.
    
        Args:
            X: Training data of shape (n_samples, n_features)
            y: Target values of shape (n_samples,)
    
        Returns:
            self: The fitted model
        """
        X = self._validate_data(X)
        y = np.array(y)
    
        # Get unique classes
        self.classes_ = np.unique(y)
    
        if len(self.classes_) != 2:
            raise ValueError(
                f"This LogisticRegression implementation only supports binary classification. "
                f"Found {len(self.classes_)} classes."
            )
    
        # Map classes to 0 and 1
        y_binary = np.where(y == self.classes_[1], 1, 0)
        y_binary = y_binary.reshape(-1, 1)
    
        n_samples, n_features = X.shape
    
        # Initialize parameters
        if self.fit_intercept:
            # Add a column of ones for the intercept
            X_with_intercept = np.hstack((np.ones((n_samples, 1)), X))
            self.weights_ = np.zeros(n_features + 1)
        else:
            X_with_intercept = X
            self.weights_ = np.zeros(n_features)
    
        # Gradient Descent
        prev_cost = float('inf')
        for i in range(self.n_iterations):
            # Calculate predictions (probability of class 1)
            linear_model = np.dot(X_with_intercept, self.weights_)
            y_pred = sigmoid(linear_model)
    
            # Calculate error
            error = y_pred - y_binary
    
            # Calculate gradients
            gradient = (1/n_samples) * np.dot(X_with_intercept.T, error)
    
            # Add regularization term if specified
            if self.penalty == "l2":
                # L2 regularization (exclude intercept from regularization)
                if self.fit_intercept:
                    reg_term = np.zeros_like(self.weights_)
                    reg_term[1:] = self.weights_[1:] / self.C
                else:
                    reg_term = self.weights_ / self.C
                gradient += reg_term
            elif self.penalty == "l1":
                # L1 regularization (exclude intercept from regularization)
                if self.fit_intercept:
                    reg_term = np.zeros_like(self.weights_)
                    reg_term[1:] = np.sign(self.weights_[1:]) / self.C
                else:
                    reg_term = np.sign(self.weights_) / self.C
                gradient += reg_term
    
            # Update parameters
>           self.weights_ -= self.learning_rate * gradient.flatten()
E           ValueError: operands could not be broadcast together with shapes (3,) (300,) (3,)

ml_library/models/classification.py:146: ValueError
----------------------------- Captured stderr call -----------------------------
2025-06-23 01:15:47 | DEBUG    | ml_library.models.base:__init__:29 | Initialized LogisticRegression with params: {'learning_rate': 0.1, 'n_iterations': 1000, 'tol': 0.0001, 'fit_intercept': True, 'penalty': 'none', 'C': 1.0}
____________________ test_logistic_regression_predict_proba ____________________

sample_classification_data = (array([[-1.25459881,  4.50714306],
       [ 2.31993942,  0.98658484],
       [-3.4398136 , -3.4400548 ],
       [-4.4...,
       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1]))

    def test_logistic_regression_predict_proba(sample_classification_data):
        """Test probability prediction of LogisticRegression."""
        X, y = sample_classification_data
    
        # Create and fit the model
        model = LogisticRegression()
>       model.fit(X, y)

tests/unit/test_models.py:129: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ml_library.models.classification.LogisticRegression object at 0x7ed81fda94c0>
X = array([[-1.25459881,  4.50714306],
       [ 2.31993942,  0.98658484],
       [-3.4398136 , -3.4400548 ],
       [-4.41... 1.33101457],
       [-1.60970209, -1.50790425],
       [ 2.25955679,  3.9711026 ],
       [ 3.87086424,  2.79875546]])
y = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
       0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,...0,
       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1])

    def fit(self, X: np.ndarray, y: np.ndarray) -> "LogisticRegression":
        """Fit the logistic regression model using gradient descent.
    
        Args:
            X: Training data of shape (n_samples, n_features)
            y: Target values of shape (n_samples,)
    
        Returns:
            self: The fitted model
        """
        X = self._validate_data(X)
        y = np.array(y)
    
        # Get unique classes
        self.classes_ = np.unique(y)
    
        if len(self.classes_) != 2:
            raise ValueError(
                f"This LogisticRegression implementation only supports binary classification. "
                f"Found {len(self.classes_)} classes."
            )
    
        # Map classes to 0 and 1
        y_binary = np.where(y == self.classes_[1], 1, 0)
        y_binary = y_binary.reshape(-1, 1)
    
        n_samples, n_features = X.shape
    
        # Initialize parameters
        if self.fit_intercept:
            # Add a column of ones for the intercept
            X_with_intercept = np.hstack((np.ones((n_samples, 1)), X))
            self.weights_ = np.zeros(n_features + 1)
        else:
            X_with_intercept = X
            self.weights_ = np.zeros(n_features)
    
        # Gradient Descent
        prev_cost = float('inf')
        for i in range(self.n_iterations):
            # Calculate predictions (probability of class 1)
            linear_model = np.dot(X_with_intercept, self.weights_)
            y_pred = sigmoid(linear_model)
    
            # Calculate error
            error = y_pred - y_binary
    
            # Calculate gradients
            gradient = (1/n_samples) * np.dot(X_with_intercept.T, error)
    
            # Add regularization term if specified
            if self.penalty == "l2":
                # L2 regularization (exclude intercept from regularization)
                if self.fit_intercept:
                    reg_term = np.zeros_like(self.weights_)
                    reg_term[1:] = self.weights_[1:] / self.C
                else:
                    reg_term = self.weights_ / self.C
                gradient += reg_term
            elif self.penalty == "l1":
                # L1 regularization (exclude intercept from regularization)
                if self.fit_intercept:
                    reg_term = np.zeros_like(self.weights_)
                    reg_term[1:] = np.sign(self.weights_[1:]) / self.C
                else:
                    reg_term = np.sign(self.weights_) / self.C
                gradient += reg_term
    
            # Update parameters
>           self.weights_ -= self.learning_rate * gradient.flatten()
E           ValueError: operands could not be broadcast together with shapes (3,) (300,) (3,)

ml_library/models/classification.py:146: ValueError
----------------------------- Captured stderr call -----------------------------
2025-06-23 01:15:47 | DEBUG    | ml_library.models.base:__init__:29 | Initialized LogisticRegression with params: {'learning_rate': 0.01, 'n_iterations': 1000, 'tol': 0.0001, 'fit_intercept': True, 'penalty': 'none', 'C': 1.0}
________________________ test_logistic_regression_score ________________________

sample_classification_data = (array([[-1.25459881,  4.50714306],
       [ 2.31993942,  0.98658484],
       [-3.4398136 , -3.4400548 ],
       [-4.4...,
       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1]))

    def test_logistic_regression_score(sample_classification_data):
        """Test the score method of LogisticRegression."""
        X, y = sample_classification_data
    
        # Create and fit the model
        model = LogisticRegression()
>       model.fit(X, y)

tests/unit/test_models.py:144: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ml_library.models.classification.LogisticRegression object at 0x7ed81fdd7e60>
X = array([[-1.25459881,  4.50714306],
       [ 2.31993942,  0.98658484],
       [-3.4398136 , -3.4400548 ],
       [-4.41... 1.33101457],
       [-1.60970209, -1.50790425],
       [ 2.25955679,  3.9711026 ],
       [ 3.87086424,  2.79875546]])
y = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
       0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,...0,
       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1])

    def fit(self, X: np.ndarray, y: np.ndarray) -> "LogisticRegression":
        """Fit the logistic regression model using gradient descent.
    
        Args:
            X: Training data of shape (n_samples, n_features)
            y: Target values of shape (n_samples,)
    
        Returns:
            self: The fitted model
        """
        X = self._validate_data(X)
        y = np.array(y)
    
        # Get unique classes
        self.classes_ = np.unique(y)
    
        if len(self.classes_) != 2:
            raise ValueError(
                f"This LogisticRegression implementation only supports binary classification. "
                f"Found {len(self.classes_)} classes."
            )
    
        # Map classes to 0 and 1
        y_binary = np.where(y == self.classes_[1], 1, 0)
        y_binary = y_binary.reshape(-1, 1)
    
        n_samples, n_features = X.shape
    
        # Initialize parameters
        if self.fit_intercept:
            # Add a column of ones for the intercept
            X_with_intercept = np.hstack((np.ones((n_samples, 1)), X))
            self.weights_ = np.zeros(n_features + 1)
        else:
            X_with_intercept = X
            self.weights_ = np.zeros(n_features)
    
        # Gradient Descent
        prev_cost = float('inf')
        for i in range(self.n_iterations):
            # Calculate predictions (probability of class 1)
            linear_model = np.dot(X_with_intercept, self.weights_)
            y_pred = sigmoid(linear_model)
    
            # Calculate error
            error = y_pred - y_binary
    
            # Calculate gradients
            gradient = (1/n_samples) * np.dot(X_with_intercept.T, error)
    
            # Add regularization term if specified
            if self.penalty == "l2":
                # L2 regularization (exclude intercept from regularization)
                if self.fit_intercept:
                    reg_term = np.zeros_like(self.weights_)
                    reg_term[1:] = self.weights_[1:] / self.C
                else:
                    reg_term = self.weights_ / self.C
                gradient += reg_term
            elif self.penalty == "l1":
                # L1 regularization (exclude intercept from regularization)
                if self.fit_intercept:
                    reg_term = np.zeros_like(self.weights_)
                    reg_term[1:] = np.sign(self.weights_[1:]) / self.C
                else:
                    reg_term = np.sign(self.weights_) / self.C
                gradient += reg_term
    
            # Update parameters
>           self.weights_ -= self.learning_rate * gradient.flatten()
E           ValueError: operands could not be broadcast together with shapes (3,) (300,) (3,)

ml_library/models/classification.py:146: ValueError
----------------------------- Captured stderr call -----------------------------
2025-06-23 01:15:47 | DEBUG    | ml_library.models.base:__init__:29 | Initialized LogisticRegression with params: {'learning_rate': 0.01, 'n_iterations': 1000, 'tol': 0.0001, 'fit_intercept': True, 'penalty': 'none', 'C': 1.0}
=============================== warnings summary ===============================
../../../.cache/pypoetry/virtualenvs/ml-library-9kDGecR2-py3.12/lib/python3.12/site-packages/pytest_cov/plugin.py:256
  /home/huy/.cache/pypoetry/virtualenvs/ml-library-9kDGecR2-py3.12/lib/python3.12/site-packages/pytest_cov/plugin.py:256: PytestDeprecationWarning: The hookimpl CovPlugin.pytest_configure_node uses old-style configuration options (marks or attributes).
  Please use the pytest.hookimpl(optionalhook=True) decorator instead
   to configure the hooks.
   See https://docs.pytest.org/en/latest/deprecations.html#configuring-hook-specs-impls-using-markers
    def pytest_configure_node(self, node):

../../../.cache/pypoetry/virtualenvs/ml-library-9kDGecR2-py3.12/lib/python3.12/site-packages/pytest_cov/plugin.py:265
  /home/huy/.cache/pypoetry/virtualenvs/ml-library-9kDGecR2-py3.12/lib/python3.12/site-packages/pytest_cov/plugin.py:265: PytestDeprecationWarning: The hookimpl CovPlugin.pytest_testnodedown uses old-style configuration options (marks or attributes).
  Please use the pytest.hookimpl(optionalhook=True) decorator instead
   to configure the hooks.
   See https://docs.pytest.org/en/latest/deprecations.html#configuring-hook-specs-impls-using-markers
    def pytest_testnodedown(self, node, error):

tests/unit/test_tree_models.py::test_classifier_fit_predict[XGBoostClassifier-model_params2]
  /home/huy/.cache/pypoetry/virtualenvs/ml-library-9kDGecR2-py3.12/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [01:15:48] WARNING: /workspace/src/learner.cc:740: 
  Parameters: { "use_label_encoder" } are not used.
  
    warnings.warn(smsg, UserWarning)

tests/unit/temp/test_imports.py::test_base_imports
  /home/huy/.cache/pypoetry/virtualenvs/ml-library-9kDGecR2-py3.12/lib/python3.12/site-packages/_pytest/python.py:198: PytestReturnNotNoneWarning: Expected None, but tests/unit/temp/test_imports.py::test_base_imports returned True, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

tests/unit/temp/test_imports.py::test_linear_models
  /home/huy/.cache/pypoetry/virtualenvs/ml-library-9kDGecR2-py3.12/lib/python3.12/site-packages/_pytest/python.py:198: PytestReturnNotNoneWarning: Expected None, but tests/unit/temp/test_imports.py::test_linear_models returned True, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

tests/unit/temp/test_imports.py::test_tree_models
  /home/huy/.cache/pypoetry/virtualenvs/ml-library-9kDGecR2-py3.12/lib/python3.12/site-packages/_pytest/python.py:198: PytestReturnNotNoneWarning: Expected None, but tests/unit/temp/test_imports.py::test_tree_models returned True, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

tests/unit/temp/test_imports.py::test_boosting_models
  /home/huy/.cache/pypoetry/virtualenvs/ml-library-9kDGecR2-py3.12/lib/python3.12/site-packages/_pytest/python.py:198: PytestReturnNotNoneWarning: Expected None, but tests/unit/temp/test_imports.py::test_boosting_models returned True, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

tests/unit/temp/test_imports.py::test_svm_models
  /home/huy/.cache/pypoetry/virtualenvs/ml-library-9kDGecR2-py3.12/lib/python3.12/site-packages/_pytest/python.py:198: PytestReturnNotNoneWarning: Expected None, but tests/unit/temp/test_imports.py::test_svm_models returned True, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

tests/unit/temp/test_imports.py::test_knn_models
  /home/huy/.cache/pypoetry/virtualenvs/ml-library-9kDGecR2-py3.12/lib/python3.12/site-packages/_pytest/python.py:198: PytestReturnNotNoneWarning: Expected None, but tests/unit/temp/test_imports.py::test_knn_models returned True, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

tests/unit/temp/test_svm.py::test_svm_regressor_init
  /home/huy/.cache/pypoetry/virtualenvs/ml-library-9kDGecR2-py3.12/lib/python3.12/site-packages/_pytest/python.py:198: PytestReturnNotNoneWarning: Expected None, but tests/unit/temp/test_svm.py::test_svm_regressor_init returned True, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

tests/unit/temp/test_svm.py::test_svm_classifier_init
  /home/huy/.cache/pypoetry/virtualenvs/ml-library-9kDGecR2-py3.12/lib/python3.12/site-packages/_pytest/python.py:198: PytestReturnNotNoneWarning: Expected None, but tests/unit/temp/test_svm.py::test_svm_classifier_init returned True, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

tests/unit/temp/test_svm.py::test_svm_regressor_fit_predict
  /home/huy/.cache/pypoetry/virtualenvs/ml-library-9kDGecR2-py3.12/lib/python3.12/site-packages/_pytest/python.py:198: PytestReturnNotNoneWarning: Expected None, but tests/unit/temp/test_svm.py::test_svm_regressor_fit_predict returned True, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

tests/unit/temp/test_svm.py::test_svm_classifier_fit_predict
  /home/huy/.cache/pypoetry/virtualenvs/ml-library-9kDGecR2-py3.12/lib/python3.12/site-packages/_pytest/python.py:198: PytestReturnNotNoneWarning: Expected None, but tests/unit/temp/test_svm.py::test_svm_classifier_fit_predict returned True, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html

---------- coverage: platform linux, python 3.12.3-final-0 -----------
Name                                       Stmts   Miss  Cover   Missing
------------------------------------------------------------------------
ml_library/__init__.py                         1      0   100%
ml_library/config/__init__.py                  2      0   100%
ml_library/config/loader.py                   40      1    98%   60
ml_library/evaluation/__init__.py              2      0   100%
ml_library/evaluation/metrics.py              48     35    27%   28, 40, 60-84, 97-98, 122-162
ml_library/inference/__init__.py               2      0   100%
ml_library/inference/predictor.py             47     32    32%   38, 55-78, 96-131
ml_library/models/__init__.py                  9      0   100%
ml_library/models/base.py                     39     19    51%   42, 54, 62-70, 85-99, 107
ml_library/models/classification.py          104     52    50%   75, 94, 111-112, 130-135, 138-143, 149-184, 195-208, 222-223, 235-239, 266, 269
ml_library/models/knn_models.py               99      9    91%   65-67, 136, 227-229, 300, 316
ml_library/models/random_forest.py           112     15    87%   65-67, 128, 144, 157, 177, 235-237, 299, 314, 330, 343, 363
ml_library/models/regression.py               64     27    58%   71-72, 90-109, 123-128, 140-147, 172, 175
ml_library/models/svm_models.py              106     11    90%   69-71, 142, 176, 242-244, 318, 340, 375
ml_library/models/tree_models.py             110     15    86%   61-63, 122, 138, 151, 170, 224-226, 286, 301, 317, 330, 349
ml_library/models/xgboost_models.py          114     15    87%   69-71, 133, 149, 162, 183, 245-247, 312, 327, 343, 356, 377
ml_library/preprocessing/__init__.py           2      2     0%   3-5
ml_library/preprocessing/transformers.py     101    101     0%   3-260
ml_library/training/__init__.py                2      0   100%
ml_library/training/trainer.py                45      3    93%   79-80, 130
ml_library/utils/__init__.py                   2      0   100%
ml_library/utils/logger.py                    13      1    92%   42
------------------------------------------------------------------------
TOTAL                                       1064    338    68%
Coverage XML written to file coverage.xml

=========================== short test summary info ============================
FAILED tests/integration/test_pipeline.py::test_regression_pipeline - ValueEr...
FAILED tests/integration/test_pipeline.py::test_classification_pipeline - Val...
FAILED tests/integration/test_pipeline.py::test_advanced_regression_pipeline[model0]
FAILED tests/integration/test_pipeline.py::test_advanced_regression_pipeline[model1]
FAILED tests/integration/test_pipeline.py::test_advanced_classification_pipeline[model0]
FAILED tests/integration/test_pipeline.py::test_advanced_classification_pipeline[model1]
FAILED tests/unit/test_models.py::test_linear_regression_fit_predict - ValueE...
FAILED tests/unit/test_models.py::test_linear_regression_score - ValueError: ...
FAILED tests/unit/test_models.py::test_logistic_regression_fit_predict - Valu...
FAILED tests/unit/test_models.py::test_logistic_regression_predict_proba - Va...
FAILED tests/unit/test_models.py::test_logistic_regression_score - ValueError...
================== 11 failed, 65 passed, 13 warnings in 2.37s ==================
